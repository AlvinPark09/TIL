## 지도_비지도 _의사결정나무모델 _분류/회귀

- 의사결정 나무 모형
  - 의사결정규칙을 나무 구조로 도표화하여 관심대상이 되는 집단을 몇 개의 소집단으로 분할하는 방식으로 분류 및 예측하는 분석 방법
    - 목표 변수가 범주형인 경우 : 분류나무
    - 목표 변수가 연속형인 경우: 회귀 나무
  - 뿌리마디(root node): 시작되는 마디로 전체 자료로 구성됨
  - 자식마디(child node): 하나의 마디로부터 분리되어 나간 2개 이상의 마디들
  - 부모마디(parent node): 주어진 마디의 상위마디
  - 끝마디(terminal node): 자식마디가 없는 마디
  - 중간마디(internal node) : 부모마디와 자식마디 모두 있는 마디
  - 깊이(depth): 뿌리마디부터 끝마디까지 중간마디의 수
- 의사결정 나무의 종류
  - CHAID, CART, C5.0, QUEST 등과 이들의 장점을 결합한 다양한 알고리즘이 있음
  - CART 분리 기준
    - 분류나무(범주형 목표변수): 지니불순도
    - 회귀나무(연속형 목표변수): 분산감소량
  - CART 특징
    - 항상 이진분리
    - 개별 특성변수 및 특성변수의 선형 결합 형태의 분리 기준도 가능
  - C4.5, C5.0의 분리기준
    - 엔트로피 불순도로 구한 정보 이득
  - C4.5, C5.0의 특징
    - 범주형 특성변수는 다진분리
    - 연속형 특성변수는 이진분리
  - CHAID 분리기준
    - 분류나무(범주형 목표변수): 카이제곱 통계량
    - 회귀나무(연속형 목표변수): ANOVA F통계량
  - CHAID 특징
    - 다진분리
    - 변수간 통계적 관계에 기반
- 의사결정나무 분석절차
  - **나무의 성장(growing)**: 각 마디에서 적절한 최적의 분리규칙을 찾아 나무를 성장시킴. 정지 규칙을 만족하는 경우는 성장을 중단
    - 상위 노드로부터 하위노드로 나무 구조를 형성하는 매 단계마다 분리규칙(어느 특성변수로 어떻게 분할할 것인가)을 선택함
    - 분리규칙의 형태(이진분할의 경우)
      - 연속형 특성변수: 분리에 사용될 특성 변수 X와 분리점 c를 이용하여 X < c면 왼쪽 자식마디, 그렇지 않으면 오른쪽 자식마디로 자료를 분리
      - 범주형 특성변수: 분리에 사용될 특성변수 X가 가지는 전체범주 중 부분집합인 A를 이용하여 X (  A면 왼쪽 자식마디, 그렇지 않으면 오른쪽 자식마디로 자료를 분리
    - 분류기준은 해당 노드에서 그 기준으로 하위노드를 분기하였을 때, 하위 노드 내에서는 동질성이 하위 노드 간에는 이질성이 가장 커지도록 선택됨
    - 분리규칙은 모든 특성변수와 그 특성변수의 모든 가능한 분리점에 대하여 불순도의 향상된 정도를 구한 뒤 불순도의 향상된 정도가 가장 큰 특성변수 및 분리점을 해당 마디에서 분리기준으로 정함
    - <u>회귀나무의 분리규칙</u>
      - 분산(=불순도)의 감소량
        - 각 그룹(자식노드)내에서의 목표변수(Y)의 분산이 작을수록, 그룹 내 이질성이 작은 것으로 볼 수 있음
        - 자식 노드로 분리했을 때 **분산의 감소량이 가장 커지도록** 하는 분리규칙을 탐색
      - ANOVA의 F통계량
        - **F값이 클수록 그룹(자식노드) 간에 평균차이가 있다는 것**이므로, 그룹 간 **이질성이 큰 것**으로 볼 수 있음.
        - **F값이 가장 커지게** 되는 분리규칙을 탐색
  - **가지치기(pruning):** 오류율(error rate)을 크게 할 위험이 높거나 부적절한 추론 규칙을 가지고 있는 가지를 제거
    - 성장이 끝난 나무의 가지를 제거하여 적당한 크기를 가지도록 함
    - 적당한 크기를 결정하는 방법은 검증용 자료(validation data)에 대한 예측 오류가 가장 작은 나무 모형을 찾는 것이 일반적이며, 이 과정은 의사결정나무 모형 알고리즘 내에 자동화되어 있는 경우가 많음
  - **타당성 평가**: 평가자료(test data)를 이용하여 의사결정나무를 평가
  - **해석 및 예측**: 구축된 나무 모형을 해석하고 분류 및 예측 모형을 설정
- **의사결정나무의 과적합 방지 방법**
  - 정지규칙(stopping rule)
    - 다음의 경우에 더 이상 분리하지 않고 나무가 성장을 멈추도록 함
      - 모든 자료의 목표변수(Y) 값이 동일할 때
      - 마디에 속하는 자료의 개수가 일정 수준보다 적을 때
      - 뿌리마디로부터 깊이가 일정 수준 이상일 때
      - 불순도의 감소량이 지정된 값보다 적을 때
  - 지나치게 많은 마디를 가지는 의사결정나무는 새로운 자료에 적용할 때 예측오차가 매우 커지는 과적합(overfitting) 상태가 됨
  - 이를 방지하기 위한 방법으로 정지규칙 또는 **가지치기** 방법을 사용함
- 의사결정나무모형 특징
  - 장점
    - 이해하기 쉬운 규칙을 생성함: if - then - else 방식
    - 특성변수 및 목표변수 둘 다 연속형, 범주형 자료 모두 취급함
    - 데이터의 전처리가 거의 필요하지 않음 (표준화, 스케일링 거의 필요 없음)
    - 이상치에 덜 민감
    - 모형에 가정이 필요 없는 비모수적 모형
  - 단점
    - 훈련결과가 불안정함 (과대적합:overfitting)
    - 모든 분할은 축에 수직임
    - 나무가 깊어질수록 과적합으로 예측력이 저하되며, 해석이 어려워짐
