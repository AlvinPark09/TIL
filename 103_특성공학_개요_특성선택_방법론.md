## 특성 공학(feature engineering): 개요, 특성 선택(Feature Selection) 방법론

- 머신 러닝 알고리즘에 사용되는 입력데이터에 해당하는 특성변수들에 대한 처리
- 특성 공학
  - 특성공간 차원축소의 필요성
    - 모델의 해석력 향상
    - 모델 훈련시간의 단축
    - 차원의 저주 방지(데이터 공간에 빈 공간이 많이 생김, 차원을 늘리면 어떤 패턴을 가지는지 등 확인을 할 수 없게됨)
    - 과적합(overfitting)(새로운 데이터가 들어왔을 때 추정이 잘 되지않는 것)에 의한 일반화 오차를 줄요 성능 향상
  - 특성공학의 방법론은 크게 **특성선택**(feature selection) 방법과 **특성 추출**(feature extraction) 방법으로 구분할 수 있음
- 특성 선택(feature selection)
  - 주어진 특성 변수들 가운데 가장 좋은 특성변수의 조합만 선택함
  - 불필요한 특성 변수를 제거함
  - filtering, wrapper, embedded 방식으로 분류할 수 있음
    - filering  
      - 특성 변수 중에 y를 설명하는데, 중요한 순서대로 ranking 같은걸 매긴 후, 순위가 높은 특성들만 선택.
      - 각 특성 변수 x와 목표 변수(y)와의 연관성을 측정한 뒤, 목표 변수를 잘 설명할 수 있는 특성 변수만을 선택하는 방식
      - x와 y의 **1:1 관계로만 연관성을 판단**
      - 연관성 파악을 위해 t-test, chi-square test, information gain 등의 지표가 활용
    - wrapper 
      - 전체 데이터에서 일부 set을 후보로 데려와 **모델에 fitting**을 해보고 결과를 평가 하고 다른 후보군을 반복적으로 돌려보는 방식
      - 다양한 특성 변수의 조합에 대해 목표변수를 예측하기 위한 알고리즘을 훈련하고, cross-validation 등의 방법으로 훈련된 모델의 예측력을 평가함. 그 결과를 비교하여 최적화된 특성변수의 조합을 찾는 방법
      - 특성변수의 **조합이 바뀔 때마다 모델을 학습**함
      - 특성변수에 **중복된 정보가 많은 경우** 이를 효과적으로 제거함
      - 대표적인 방법으로 순차탐색법인 forward selection, backward selection, stepwise selection 등이 있음
    - embedded   
      - 모델이 자체적으로 어떤 변수를 쓸 것인지 해결을 해주는 모델을 쓰는 방식. 변수 선택 기능이 들어가있는 함수 사용
      - wrapper 방식은 모든 특성변수 조합에 대한 학습을 마친 결과를 비교하는데 비해, embedded 방식은 학습 과정에서 최적화된 변수를 선택한다는 점에서 차이가 있따
      - 대표적인 방법으로는 특성변수에 규제를 가하는 방식인 Ridge, Lasso, Elastic net 등이 있음
    - filter와 wrapper 장단점 비교
      - filter 장점: 계산비용이 적고 속도가 빠름
      - 단점: 특성 변수간의 상호작용을 고려하지 않음
      - wrapper 장점: 특성 변수간의 상호 작용을 고려함, 주어진 학습 알고리즘에 대해 항상 최적의 특성변수 조합을 차증ㅁ
      - 단점: 모델 학습이 필요하여 계산 비용이 크고 속도가 느림. 과적합(overfitting) 가능성
- 특성 추출법
  - 주성분분석(PCA: principle component analysis)
    - 서로 **연관**되어 있는 변수들이 관찰되었을 때, 이 변수들이 **전체적으로 가지고 있는 정보들을 최대한 확보**하는 **적은 수**의 새로운 변수(주성분,PC)를 생성하는 방법
    - 주성분 분석 목적
      - 자료에서 변동이 큰 축을 탐색함
      - 변수들에 담긴 정보의 손실을 최소화하면서 차원을 축소함
      - 서로 상관이 없거나 독립적인 새로운 변수인 주성분을 통해 데이터의 해석을 용이하게 함
    - 주성분 분석에 관한 기하학적 의미
      - 주성분축은 원래 변수들의 좌표축이 직교 회전 변환된 것으로 해석할 수 있음
        - 첫번째 주성분 축은 데이터의 변동이 가장 커지는 축임
        - 두번째 주성분 축은 첫번째 주성분 축과 직교하며 첫번째 주성분축 다음으로 데이터의 변동이 큰 축을 나타냄
        - 각 관찰치 별 주성분 점수는 대응하는 원 자료 값들의 주성분 좌표축에서의 좌표 값에 해당
        - 자료들의 공분산 행렬이 대각행렬이 되도록 회전한 것으로 해석할 수 있음
  - 특성값분해(SVD)
  - LDA
  - NMF

